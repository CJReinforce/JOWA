_target_: models.TransformerConfig
tokens_per_block: 37
max_blocks: 8  # 20
max_tasks: 60
attention: 'causal'
num_layers: 6  # 4  # 12
num_heads: 12  # 8  # 12
embed_dim: 768  # 512  # 768
embed_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1
