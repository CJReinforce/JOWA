defaults:
  - _self_
  - tokenizer: default
  - transformer: JOWA_150M
  - critic_head: JOWA_150M

wandb:
  mode: online
  project: JOWA
  entity: null
  name: planning_${training.action.use_imagination}_bw_${training.action.planning_beam_width}_h_${training.action.planning_horizon}_q_penalty_${training.action.q_penalty}_real_bs_${training.world.batch_size}_imagined_bs_${training.world.imagine_batch_size}_wm_lr_${training.world.learning_rate}_critic_lr_${training.world.critic_lr}
  group: null
  tags: null
  notes: null

initialization:
  load_tokenizer: null  # path of ckpt
  load_optimizer_tokenizer: False
  load_world_model: ${initialization.load_tokenizer}
  load_world_model_name: JOWA_150M
  load_optimizer_world_model: False
  load_start_epoch: False

common:
  env: [
    Phoenix, 
    Centipede, 
    SpaceInvaders, 
    Carnival, 
    NameThisGame, 
    Assault, 
    Atlantis, 
    BeamRider, 
    Seaquest, 
    TimePilot, 
    Berzerk, 
    Zaxxon, 
    DemonAttack, 
    ChopperCommand, 
    StarGunner
  ]
  epochs: 1000
  device: cuda
  do_checkpoint: True  # whether save optimizer
  seed: 0
  sequence_length: ${world_model.max_blocks}

training:
  should: True
  log_interval: 500
  max_ckpts: 20
  dtype: bfloat16
  tokenizer:
    should: True
    learning_rate: 0.00005
    batch_size: 256  # per rank
    max_grad_norm: 1.0
  world:
    learning_rate: 0.00005
    # batch size will be this value when not using imagnation data, 
    # otherwise will be this value minus ${training.action.batch_size_in_imagination}
    batch_size: 64
    max_grad_norm: 1.0
    weight_decay: 0.01
    imagine_horizon: ${common.sequence_length}
  action:
    train_critic: True
    train_critic_after_n_steps: 0
    use_imagination: False
    imagine_after_n_steps: 3000
    learning_rate: 0.00005
    gamma: 0.99  # discount factor
    # Q distribution
    vmin: -10
    vmax: 30
    num_atoms: 51
    # loss
    td_loss: c51  # in [c51, mse]
    q_penalty: combo  # in [cql, combo, null]
    cql_weight: 0.1  # for cql and combo
    supervised_weight: 0.1  # only works when training critic head
    # ablation
    use_rem: False
    use_task_embed: True
    q_loss_backwards_wm: True
    # target Q head
    target_update_frequency: 1000  # hard update
    # imagination / data augmentation
    batch_size_in_imagination: 8
    policy_in_imagination: 'planning'  # in ['planning', 'epsilon-greedy']
    # planning in imagination
    planning_beam_width: 2
    planning_horizon: 2
    # epsilon-greedy policy in imagination
    initial_epsilon: 1.0
    final_epsilon: 0.1
    final_training_steps: 20000

evaluation:
  should: True
  world:
    save_reconstructions: True
  action:
    step_frequency: 125000
    epoch_frequency: 1
  env:
    env_name: DemonAttack  # single env eval during training
    num_eval_episodes: 1  # per rank
    num_given_steps: 2
    max_time: 180  # seconds
    fps: 15
    header: False  # verbose info
    do_reconstruction: False  # save reconstruction images with original images
    save_mode: False  # save mp4 video

datasets:
  train:
    num_of_workers: 32

hydra:
  run:
    dir: .
  output_subdir: null