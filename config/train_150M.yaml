defaults:
  - _self_
  - tokenizer: default
  - transformer: JOWA_150M
  - critic_head: JOWA_150M

wandb:
  mode: online
  project: JOWA
  entity: null
  name: batch_size_${training.world.batch_size}_wm_lr_${training.world.first_stage.learning_rate}_critic_lr_${training.action.learning_rate}_num_q_${critic_head.num_q}
  group: pretraining
  tags: null
  notes: null

initialization:
  load_tokenizer: null  # path of ckpt
  load_optimizer_tokenizer: False
  load_jowa: ${initialization.load_tokenizer}
  load_jowa_name: JOWA_150M
  load_optimizer_jowa: False
  load_start_epoch: False

common:
  envs: [
    Phoenix, 
    Centipede, 
    SpaceInvaders, 
    Carnival, 
    NameThisGame, 
    Assault, 
    # Atlantis, 
    # BeamRider, 
    # Seaquest, 
    # TimePilot, 
    # Berzerk, 
    # Zaxxon, 
    # DemonAttack, 
    # ChopperCommand, 
    # StarGunner
  ]
  epochs: 100
  steps: 1.75e6  # gradient steps
  device: cuda
  do_checkpoint: True  # whether save optimizer
  seed: 0
  sequence_length: ${transformer.max_blocks}

training:
  should: True
  log_interval: 1000
  max_ckpts: 5
  dtype: bfloat16
  tokenizer:
    first_stage:
      should: True
    second_stage:
      should: False
    learning_rate: 0.0001
    batch_size: 2048  # all batch_size in this file is the num in total, not per rank
    max_grad_norm: 1.0
  world:
    first_stage:
      learning_rate: 0.0001
    second_stage:
      learning_rate: 0.00005
    # batch size will be this value when not using imagnation data, 
    # otherwise will be this value minus ${training.action.batch_size_in_imagination}
    batch_size: 512
    max_grad_norm: 1.0
    weight_decay: 0.01
  action:
    learning_rate: ${training.world.second_stage.learning_rate}
    train_critic_after_n_steps: 0.25e6  # means switching stages
    use_imagination: False
    imagine_after_n_steps: 1e10  # imagine_after_n_steps > train_critic_after_n_steps
    imagine_horizon: ${common.sequence_length}
    gamma: 0.99  # discount factor
    # Q distribution
    vmin: -10
    vmax: 30
    num_atoms: 51
    # loss
    td_loss: c51  # in [c51, mse]
    q_penalty: cql  # in [cql, combo, null]
    cql_weight: 0.1  # for cql and combo
    supervised_weight: 0.1  # only works when training critic head
    # ablation
    use_rem: False
    use_task_embed: True
    q_loss_backwards_wm: True
    # target Q head
    target_update_frequency: 1000  # hard update
    # imagination / data augmentation
    batch_size_in_imagination: 512
    policy_in_imagination: 'planning'  # in ['planning', 'epsilon-greedy']
    # planning in imagination
    planning_beam_width: 2
    planning_horizon: 2
    # epsilon-greedy policy in imagination
    initial_epsilon: 1.0
    final_epsilon: 0.1
    decay_steps: 20000

evaluation:
  should: True
  world:
    save_reconstructions: True
  action:
    step_frequency: 125000
    epoch_frequency: 1
  env:
    env_name: Assault  # single env eval during training
    buffer_size: 6

    num_eval_episodes: 1  # per rank
    max_time: 180  # seconds
    fps: 15
    header: False  # verbose info
    do_reconstruction: False  # save reconstruction images with original images
    save_mode: False  # save mp4 video

datasets:
  train:
    num_of_workers: 64  # per node, not per rank

hydra:
  run:
    dir: .
  output_subdir: null